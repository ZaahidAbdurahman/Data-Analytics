# Data-Analytics

# Module 1

# CHAPTER 1

 1. What the role of a Data Analyst is and the scope of data analytics.

 2. The process of data analytics.

 3. The techniques of data analytics.

 4. The role of data governance.

# WHAT IS DATA ANALYTICS

 - The ultimate role of a data analyst is to transform raw data into actionable insights that guide decision-making processes within an organization. 

 - This involves several key responsibilities and skills.

 - A data analyst helps an organization make informed decisions that can improve operational efficiencies, drive business strategies, and create a competitive advantage. 

 - The goal is to contribute to the organization's success by turning data into a valuable asset that informs and drives decision-making.

1. Data Collection and Preparation:

  - Sourcing data from various channels, including databases, spreadsheets, and external sources,
 
  - Cleaning and organizing the data to ensure it is accurate, consistent, and ready for analysis.

2. Data Analysis:

  - Employing statistical methods, machine learning techniques, or other analytic tools to interpret data,
 
  - Identifying trends, patterns, and correlations that might not be immediately obvious.

3. Data Visualization and Storytelling:

  - Creating visual representations of the data, such as charts, graphs, and dashboards, to make complex information easily understandable,

4. Decision Support:

  - Making recommendations based on data-driven insights to help guide business decisions,
 
  - Providing context around the data, including potential implications and future trends.

5. Collaboration and Communication:

  - Working closely with other departments, such as marketing, finance, and operations, to understand their data needs and provide insights,
 
  - Effectively communicating complex data findings in a clear and concise manner to non-technical stakeholders,

6. Continuous Learning and Adaptation:

  - Keeping up-to-date with the latest industry trends, tools, and technologies in data analysis.
 
  - Adapting to new types of data and analytical methods as the organization's needs evolve.

 Analytics is made possible by modern data, storage, and computing capabilities.

# THE ANALYTICS PROCESS

  Analysts working with data move through a series of different steps as they seek to gain business value from their organization's data. 

 - Data Acquisition

 - Cleaning and Manipulation

 - Analysis

 - Visualization

 - Reporting and Communication

 While we describe the steps of the analytics process as a series of sequential actions.

 It is more accurate to think of them as a set of interrelated actions that may be revisited frequently while working with a dataset.

# Analytics Techniques:

  Analysts use a variety of techniques to draw conclusions from the data at their disposal.

#  Descriptive Analytics

 - Descriptive analytics focuses on exploring and interpreting historical data to provide a comprehensive understanding of past events and trends. 

#  Predictive Analytics

 - Predictive analytics is particularly valuable for businesses when they want to stay ahead of future trends, behaviors, and outcomes.

# Prescriptive Analytics

 - Prescriptive analytics is most valuable for businesses when they are seeking to predict future outcomes and determine the best course of action to achieve specific objectives. 

# DATA GOVERNANCE

 - Data governance programs ensure that the organization has high-quality data and is able to effectively control that data.

# ANALYTICS TOOLS

 - Software helps analysts work through each one of the phases of the analytics process. 

 - These tools automate much of the heavy lifting of data analysis, improving the analyst's ability to acquire, clean, manipulate, visualize, and analyze data. 

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/f09da0c1-e0ba-4ae7-a6e6-d04bd4a18d85)

Data Analytics Excel Spreadsheet



# CHAPTER 2

 1. Understand Domain 1.0: Data Concepts and Environments

 2. Compare and contrast different data types

 3. Compare and contrast common data structures and file formats

# EXPLORING DATA TYPES

 - A data element is an attribute about a person, place, or thing containing data within a range of values.
 
 - Data elements also describe characteristics of activities, including orders, transactions, and events. 




![Screenshot (1)](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/936c362a-4aab-4396-961d-2a7cf267163f)



# TABULAR DATA

 - Tabular data is data organized into a table, made up of columns and rows. 

 - A table represents information about a single topic.

 - Each column represents a uniquely named field within a table, also called a variable, about a single characteristic. 

 - The contents of each column contain values for the data element as defined by the column header.

 - Spreadsheets, including Microsoft Excel, Google Sheets, and Apple Numbers, are practical tools for representing tabular data

 - A relational database management system (RDMS), commonly called a database, extends the tabular model.

# Structured Data Types

 - Structured data is tabular in nature and organized into rows and columns. 

 - Structured data is what typically comes to mind when looking at a spreadsheet. With clearly defined column headings, spreadsheets are easy to work with and understand. 

 - In a spreadsheet, cells are where columns and rows intersect.
 
![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/83f2a93a-34b5-4530-9ea7-f7de6dd03c4b)

# CHARACTER

 - The character data type limits data entry to only valid characters. 

 - Characters can include the alphabet that you might see on your keyboard, as well as numbers. 

 - Depending on your needs, multiple data types are available that can enforce character limits.

# Alphanumeric

 - Is the most widely used data type for storing character-based data. As the name implies, alphanumeric is appropriate when a data element consists of both numbers and letters.

 - The alphanumeric data type is ideal for storing product stock-keeping units (SKUs).

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/0c408cfc-4acd-4bf6-8006-fd81b071bab9)



Table 2.2: Selected character data types and maximum size


# Character Sets

 - When considering alphanumeric and text data types, you need to think about the character set you are using to input and store data when using a database. 

 - Databases use character sets to map, or encode, data and store it digitally.

# Numeric

 - When numbers exclusively make up values for a data attribute, numeric becomes the data type of choice.

# Whole Number

 - The integer, and all its subtypes, are for storing whole numbers. 

# Rational Number

 - In all its variants, the numeric data type is for rational numbers that include a decimal point.

# Date and Time

 - Gathered together under the broad category of date, day of year and time of day are data elements that appear with great frequency.

# Currency

 - Many people use spreadsheets to manage their finances. 

# Strong And Weak Typing

 - Data types define values placed in columns. 

 - Strong typing is when technology rigidly enforces data types. 

 - Databases, discussed in Chapter 3, use strong typing. A database column defined as numeric only accepts numerical values. You will get an error if you attempt to enter characters into a numeric column.

# Unstructured Data Types

 While much of the data we use to record transactions is highly structured, most of the world's data is unstructured. 

 - Unstructured data is any type of data that does not fit neatly into the tabular model. 

 - Examples of unstructured data include digital images, audio recordings, video recordings, and open-ended survey responses. 

 - Binary data types are one of the most common data types for storing unstructured data.

 - Audio data can come from a variety of sources. 

 - Image data can come from a variety of sources.

 - Video data is growing at a similar pace to image data.

 - Large Text is another data type

  # Categories of Data

 - We try to fit data into structured and unstructured categories. 

 - The reality is that the world is not black and white, and not all data fits neatly into structured and unstructured categories. 

 - Semi-structured data represents the space between structured spreadsheets and unstructured videos.

 # Quantitative vs. Qualitative Data

 - Quantitative data consists of numeric values. Data elements whose values come from counting or measuring are quantitative.

 - Qualitative data consists of frequent text values. Data elements whose values describe characteristics, traits, and attitudes are all qualitative.

 # Discrete vs. Continuous Data

 # Discrete Data

 - Numeric data comes in two different forms: discrete and continuous.

 - Discrete data is that it represents measurements that can't be subdivided.

 - You may intuitively think of discrete data as using whole numbers,

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/d2f5c70a-ee86-4e39-8af4-65cadcffe27c)


# Continuous Data

 - Continuous data refers to data that can be measured.
  
 - This data has values that are not fixed and have an infinite number of possible values.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3090ea0c-2670-485e-88d0-064cb0b27503)



# Categorical Data 

 - In addition to quantitative, numeric data, there is categorical data. 

 - Text data with a known, finite number of categories is categorical.

# Dimensional Data

 - Dimensional modeling is an approach to arranging data to facilitate analysis. 

 - Dimensional modeling organizes data into fact tables and dimension tables.

# Common Data Structures

 - In order to facilitate analysis, data needs to be stored in a consistent, organized manner.

 - When considering structured data, several concepts and standards inform how to organize date.

# Structured Data

 - Tabular data is structured data, with values stored in a consistent, defined manner, organized into columns and rows. 

 - Data is consistent when all entries in a column contain the same type of value.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/2db99357-14be-49cf-b037-a924694858b0)

  Figure 2.19: Data Entry Error

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/6f96fe5f-9485-4d98-a4d7-976bbd2af0b4)

 Figure 2.20: Data entry error identified in a summary


# Unstructured Data

 - Unstructured data is qualitative, describing the characteristics of an event or an object. 

 - Images, phrases, audio or video recordings, and descriptive text are all examples of unstructured data.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3e198f8c-15fc-4044-8120-1624aade7bd9)

 Figure 2.22: Unstructured data: log entry

# Semi-Structured Data

 - Semi-structured data is data that has structure and that is not tabular. 

 - Email is a well-known example of semi-structured data.

 - Every email message has structural components, including recipient, sender, subject, date, and time.

# Common File Formats

 - Common file formats facilitate data exchange and tool interoperability. 

 - Several file formats have emerged as standards and are widely adopted. 

 - As a modern data analyst, you will need to recognize all of these formats and be familiar with common use cases for each type.

# Text Files

 - Text files are one of the most commonly used data file formats.

 - As the name implies, they consist of plain text and are limited in scope to alphanumeric data. 

 - When a file is comma-delimited, it is known as a comma-separated values (CSV) file. 

 - Similarly, when a file is tab-delimited, it is called a tab-separated values (TSV) file.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/9d13be63-a54c-4de6-93fc-827ea1e2c368)

 Figure 2.25: Exporting as CSV or TSV

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/12e15ede-d994-4efb-928e-4caf88a1fba3)

 Figure 2.26: Contents of a CSV file

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/bf1af23c-2d7c-4368-a588-15277a5620e5)

 Figure 2.27: Semi-structured CSV

# Fixed-Width Files

 - Before it was common to use delimited files with variable-length columns, flat files were fixed-width. 

 - As illustrated in Figure 2.28. Fixed-width files are more laborious to create since they require a few extra steps.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/ab16021e-fbdc-4a63-9f6c-bef7c4ef07c7)

 Figure 2.28: Fixed-width file

# JavaScript Object Notation

 - JavaScript Object Notation (JSON) is an open standard file format, designed to add structure to a text file without incurring significant overhead. 

 - One of its design principles is that JSON is easily readable by people and easily parsed by modern programming languages.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/0d0808b2-a1db-4985-8a10-50e5917e24af)

 Figure 2.29: Pet data JSON example


# Extensible Markup Language (XML)

 - Extensible Markup Language (XML) is a markup language that facilitates structuring data in a text file. 

 - While conceptually similar to JSON, XML incurs more overhead because it makes extensive use of tags. Tags describe a data element and enclose each value for each data element. 

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/67b83e15-aee6-417f-b808-55e4f9f09663)

 Figure 2.32: Representing a single animal in XML

# HyperText Markup Language (HTML)

 - HyperText Markup Language (HTML) is a markup language for documents designed to be displayed in a web browser. 

 - HTML pages serve as the foundation for how people interact with the World Wide Web. Similar to XML, HTML is a tag-based language.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/90bd006e-7e77-4559-bd3f-9ded416a504a)

 Figure 2.34: HTML table in a browser

# Chapter 2 Summary

 1. Consider the values of what you will store before selecting data types. 

   - Data types are used to store different kinds of values. 

   - When dealing with numeric information, the best option is a numeric data type that can accommodate decimals.


 2. Know that you can format data after storing it.

  - While data types determine how data gets stored, formatting data governs how data will be displayed to a person.


 3. Consider the absolute limits of values that you will use before selecting data types.

   - When selecting data types, consider the range of values that a data element can contain.


4. Explain the differences between structured and unstructured data

  - Individual data elements fall along the structured data continuum. 

  - At one end, there is highly structured, rectangular data.


5. Understand the differences in common file formats

  - Common file formats make it easy for people to read a file's contents and facilitate interoperability between tools.

  - Delimiters separate variable-length fields in a file.

# Module 2

# Chapter 3

# Databases and Data Acquisition

In this chapter, we will learn about:

 - Data Concepts and Environments

 - Identify basic concepts of data schemas and dimensions

 - Understanding the domain of Data Mining

 - Explain data acquisition concepts

 - Explain common techniques for data manipulation and query optimization

# Exploring Databases

# Relational database 

 - Traditional workloads and systems

 - Excels at storing and processing structured data 

 - Consistency rollback

 - Stored procedures

 - Security ,OLAP

 - Ease of backup

 - Real Time data and detailed

 - Relational databases are pieces of software that let you make an operational system out of an ERD

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/1546d49e-3690-420f-9791-15126342e2af)
 
 Figure 3.6: Database schema


# Nonrelational database 

 - Developing application 

 -  Unstructured

 - Key values , Column store , Graph Databases , Doc store Databases

 - Non-relational databases are ideal for handling large volumes of data and are highly efficient.

 - A nonrelational database does not have a predefined structure based on tabular data.

 - Flexibility, Scalability, Cost effectiveness

# Entity Relationship Diagram

- The entity relationship diagram (ERD) is a visual artifact of the data modeling process. 

- It shows the connection between related entities.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3b0178fa-a7a0-4cd3-ac43-bd461962b06e)

  Figure 3.3: ERD line endings

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3edcc5bc-6e5e-4429-8597-d088486b0c55)
 
  Figure 3.4: Entity Relationship Diagram


 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/e5ba70a2-c48e-4687-a4ba-cc1ef9a4110c)

  Figure 3.11: Foreign key data constraint

# Normalization

- Normalization is a process for structuring a database in a way that minimizes duplication of data. 

- First normal form (1NF) Is when every row in a table is unique and every column contains a unique value. 

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/b2d2f043-685e-44df-8ae3-beb65fc17c1d)

Figure 3.15: Data in first normal form

- Second Form(2NF)In addition to each row being unique, 2NF applies an additional rule stating that all nonprimary key values must depend on the entire primary key.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/a88691cb-5cec-44e2-8731-9cb693793a4b)

 Figure 3.16: Data in second normal form

- Third Form (3NF)Adding a rule stating all columns must depend on only the primary key.

- Evolving Figure 3.16 into 3NF results in Figure 3.17. 

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/bbe31969-c732-4baa-b56c-cadc885e478a)

Figure 3.17: Data in third normal form


# Online Transactional Processing

- OLTP systems handle the transactions we encounter every day. 

![Screenshot (4)](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/e7e65247-2a4a-4b0b-950e-8c2464d3fa1b)

 Figure 3.14: Vet clinic transactional schema

# Online Analytical Processing

- OLAP systems focus on the ability of organizations to analyze data.

- While OLAP and OLTP databases can both use relational database technology, their structures are fundamentally different.

- OLTP databases need to balance transactional read and write performance, resulting in a highly normalized design.

- Typically, OLTP databases are in 3NF.

# Schema Concepts

- The design of a database schema depends on the purpose it serves. 

- Transactional systems require highly normalized databases, whereas a denormalized design is more appropriate for analytical systems.

 A database used for transactions but slower at querying large amounts of data.

 A data mart is a subset of a data warehouse.

 A data lake stores raw data in its native format instead of conforming to a relational database structure. 
  
  - Desgined to capture data from structured, unstructured and semi structured.
  
  - Used for large amounts of data

 A data warehouse is used for analytics and reporting.

 - Has no interference and faster processing.
 
 - Designed for OLAP
 
 - Data summarized

 The star schema design to facilitate analytical processing gets its name from what the schema looks like when looking at its entity relationship diagram, as Figure 3.20 illustrates.

 Star schemas are denormalized to improve read performance over large datasets.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/a191fb92-af52-43d1-a69e-b159bc7d90e0)

 Figure 3.20: Star schema example


 A snowflake schema query is more complex than the equivalent query in a star schema. 

- Part of the trade-off is that a snowflake schema requires less storage space than a star schema.

- Another design pattern for data warehousing is the snowflake schema. As its name implies, the schema diagram looks like a snowflake.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/6296e988-773e-4608-9ef3-5d9c8a8f50fe)

  Figure 3.22: Snowflake example


 # Dimensionality

 - Dimensionality refers to the number of attributes a table has. 

 - The greater the number of attributes, the higher the dimensionality.

# Handling Dimensionality

- There are multiple ways to design dimensions. Table 3.5 illustrates the start and end date approach. 

- An understanding of this method is required to write a query to retrieve the current price.

  ![Screenshot (5)](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/5687aae9-55c9-4639-8d96-13e3a4338cf4)

   Table 3.5: Product dimension


# Data Acquisition Concepts

- To perform analytics, you need data. 

- Data can come from internal systems you operate, or you can obtain it from third-party sources.

# Integration 

- Data from transactional systems flow into data warehouses and data marts for analysis.

# ETL and ELT

 Extract:  In the first phase, you extract data from the source system and place it in a staging area. 

 The goal of the extract phase is to move data from a relational database into a flat file as quickly as possible.

 Transform:  The second phase transforms the data. The goal is to reformat the data from its transactional structure to the data warehouse's analytical design.

 Load:  The purpose of the load phase is to ensure data gets into the analytical system as quickly as possible.

- Extract, load, and transform (ELT) is a variant of ETL. With ELT, data is extracted from a source database and loaded directly into the data warehouse. 
 
- Once the extract and load phases are complete, the transformation phase gets underway.

# ETL Vendors

 - Whether you choose ETL or ELT for loading your data warehouse, you don't have to write transformations by hand. 
 
 - Many products support both ETL and ELT.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/eef1976c-31a7-4d84-8b40-a531a1eddbb9)

  Figure 3.23: Delta load example

  An initial load occurs the first time data is put into a data warehouse.
 
  Figure 3.23 illustrates a monthly delta-load approach that continues throughout the year.

#  Data collection methods

 # Application Programming Interfaces (API)

- An application programming interface (API) is a structured method for computer systems to exchange information. 
 
- APIs provide a consistent interface to calling applications, regardless of the internal database structure. 

#  Web Services

- Many smartphone applications need a network connection, either cellular or Wi-Fi, to work correctly. 

 # Surverys

 - One way to collect data directly from your customers is by conducting a survey.
 
 - The most simplistic surveys consist of one question and indicate customer satisfaction.

#  Observation

- Observation is the act of collecting primary source data, from either people or machines. 

#  Sampling
 
 - Regardless of the data acquisition approach, you may end up with more data than is practical to manipulate.

  Survey Tools

- Instead of designing a custom application to collect survey data, several survey products let you design complex surveys without worrying about building a database. 

#  Web Scrapping

- Some of the data you want may not be available internally as an API or publicly via a web service.

# Working With Data

# CRUD

- Create new data.

- Read existing data.

- Update existing data.

- Delete existing data.

![Screenshot (6)](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/524c1543-8154-40cf-8764-8ac89a8f3995)

 Table 3.8: Data manipulation in SQL
 
 Augmenting data from your transactional systems with external data is an excellent way to improve the analytical capabilities of your organization. 

 # Filtering

- Examining a large table in its entirety provides insight into the overall population. 

# Sorting

- When querying a database, you frequently specify the order in which you want your results to return.

# Date Functions

- Date columns also appear in transactional systems. 
 
- Storing date information about an event facilitates analysis across time.

- The most important thing to note is that you have to understand the database platform you are using and how that platform handles dates and times.

# Logical Functions

- Logical functions can make data substitutions when retrieving data. 

- Remember that a SELECT statement only retrieves data. 

- The data in the underlying tables do not change when a SELECT runs.

  
 Boolean Expressions: The expression must return either TRUE or FALSE.

 True: Value return true if return value

 False: Value return false if return value

# Aggregate Functions

- Summarized data helps answer questions that executives have, and aggregate functions are an easy way to summarize data. 

- Aggregate functions summarize a query's data and return a single value.

# System Functions

- Each database platform offers functions that expose data about the database itself. 

- One of the most frequently used system functions returns the current date.

# Query Optimization

- Writing an SQL query is straightforward. Writing a SQL query that efficiently does what you intend can be more difficult. 
 
- There are several factors to consider when creating well-performing SQL.

 Paremetrization

 - Whenever a SQL query executes, the database has to parse the query.
 
 - Parsing translates the human-readable SQL into code the database understands. 

 Indexing

- When retrieving data from a table, the database has to scan each row until it finds the ones that match the filters in the WHERE clause. 

- The process of looking at each row is called a full table scan.

 Execution plan

- An execution plan shows the details of how a database runs a specific query. 

 Data Subsets and temperature tables

- When dealing with large data volumes, you may want to work with a subset of records.

# Chapter 3 Summary 

Describe the characteristics of OLTP and OLAP systems

- The two main categories of relational databases are transactional (OLTP) and analytical (OLAP). 

- Transactional systems use highly normalized schema design, which allows for database reads and writes to perform well. 

- Analytical systems are denormalized and commonly have a star or snowflake schema.

Describe approaches for handling dimensionality 

- It is crucial to keep track of how data changes over time to perform historical analysis. 

- Although an effective date approach is valid, the SQL queries to retrieve a value at a specific point in time are complex.

  Understand integration and how to populate a data warehouse

- The more data an organization has, the more impactful the analysis it can conduct. 

- The extract, transform, and load (ETL) process copies data from transactional to analytical databases.

 Differentiate between data collection methods

- Data can come from a variety of sources.

- An organization may scrape websites or use publicly available databases to augment its data.

  Describe how to manipulate data and optimize queries

 - Analytical databases store massive amounts of data. 
 
 - Manipulating the entire dataset for analysis is frequently infeasible. 
 
 - To efficiently analyze data, understand that SQL has the power to filter, sort, and aggregate  data.

# Chapter 4

# Data Quality

In this chapter, we will learn about:

- Understanding the challenges of data quality. 

- Identifying common reasons for cleansing and profiling datasets.

- Executing data manipulation techniques.

- Applying data quality control concepts.

# Duplicate Data

- Duplicate data occurs when data representing the same transaction is accidentally duplicated within a system. 

- Suppose you want to open a spreadsheet on your local computer. To open the spreadsheet, you locate the file and double-click it. 

- This method of opening documents establishes muscle memory that associates double-clicking with the desired action.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/072b5194-8ff9-4a89-837a-6eb16a169747)

  Figure 4.1: Duplicate data resolution process

# Redundant Data 

- While duplicate data typically comes from accidental data entry.

- Redundant data happens when the same data elements exist in multiple places within a system.

- For example, multiple source systems that perform different business functions and use shared data elements create the conditions for data redundancy.



![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/99cd6472-8647-4321-aa87-afe7a8a1b058)

 Figure 4.3: Resolving redundancy with an integrated ETL process


 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/d9ec8926-9f90-460e-9290-f7aae353cfcb)

 
 Figure 4.4 illustrates a poorly designed transactional table for storing billing information.

# Missing Values

- Another issue that impacts data quality is the concept of missing values. 

- Missing values occur when you expect an attribute to contain data but nothing is there. Missing values are also known as null values.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/d00a9047-f741-410f-993e-5682172c003a)

  Figure 4.7: Missing temperature value


# Invalid Data

- Invalid data are values outside the valid range for a given attribute. 

- An invalid value violates a business rule instead of having an incorrect data type. 

- As such, you have to understand the context of a system to determine whether or not a value is invalid.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/d9aea00d-4776-4dbb-ab2e-dc0de5d15d32)

 Figure 4.8: Invalid temperature value


# Nonparametric Data

 - Nonparametric data is data collected from categorical variables, which you read about in Chapter 2: Understanding Data. 
 
 - Sometimes the categories indicate differentiation, and sometimes they have a rank order associated with them.

# Data Outliers

- A data outlier is a value that differs significantly from other observations in a dataset.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/e4375fff-538e-46d1-832b-8d8c2ab1136e)

   Figure 4.10: Real estate sales outlier

# Specification Mismatch

- A specification describes the target value for a component. 

- A specification mismatch occurs when an individual component's characteristics are beyond the range of acceptable values.

  Data Type Validation

- Data type validation ensures that values in a dataset have a consistent data type.

- The primary keys for both the Manufacturer and Model expect integer values, while the Manufacturer_Name and Model_Name are characters.

- Consider the schema excerpt in Figure 4.11.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/11c361c5-6de5-49d7-b741-76362de2ab9d)

 Figure 4.11: Automotive schema excerpt

# Data Manipulation Techniques

- There are several potential issues to be aware of and account for when working with data. 

- With those possibilities in mind, let's explore some of the data manipulation techniques you can use to resolve potential data quality issues.

 # Recoding Data

 - Recoding data is a technique you can use to map original values for a variable into new values to facilitate analysis. 
 
 - Recoding groups data into multiple categories, creating a categorical variable.
 
 - A categorical variable is either nominal or ordinal.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/12032c2a-a3a3-459a-b277-339342c904a5)

 Figure 4.14: Recoded patient pain data

# Derived Variables

- A derived variable is a new variable resulting from a calculation on an existing variable. 

- In the case of the recoded data in Figure 4.14, the Pain_Category categorical variable is an example of a derived variable. 

- However, derived variables don't have to be categorical.

# Data Merge

- A data merge uses a common variable to combine multiple datasets with different structures into a single dataset. 

- Merging data improves data quality by adding new variables to your existing data.

- 	Additional variables make for a richer dataset, which positively impacts the quality of your analysis.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/aa454fe9-a0ce-46c1-86af-727329599e78)

Figure 4.18: ETL and the data merge approach

# Data Blending

- Data blending combines multiple sources of data into a single dataset at the reporting layer. 

- While data blending is conceptually similar to the extract, transform, and load process in Chapter 3, there is a crucial difference.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/13bc92a9-8ee2-4caa-8c8d-1f632981dab4)

  Figure 4.20: Data blending

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/9621a184-cb23-46fe-b20f-30ec33d82141)

  Figure 4.19: Extract, transform, and load process

  - Consider the illustration in Figure 4.19. With the traditional ETL workflow, the analyst needs to understand the data warehouse's structure to create a visualization.

  - For routine analysis, such as weekly profitability, the ETL approach works well.
 
  # Concatenation

 - Concatenation is the merging of separate variables into a single variable. 

 - Concatenation is a highly effective technique when dealing with a source system that stores components of a single variable in multiple columns. 

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/b26eafc1-beaf-471f-923a-0168be69b7ad)

 Figure 4.21: Creating a date variable with concatenation

# Data Append
 
 - A data append combines multiple data sources with the same structure, resulting in a new dataset containing all the rows from the original datasets.
 
 - When appending data, you save the result as a new dataset for ongoing analysis.

 # Imputation

- Imputation is a technique for dealing with missing values by replacing them with substitutes.

- When merging multiple data sources, you may end up with a dataset with many nulls in a given column.

# Reduction

- When dealing with big data, it is frequently unfeasible and inefficient to manipulate the entire dataset during analysis. 

- Reduction is the process of shrinking an extensive dataset without negatively impacting its analytical value.

 # Dimensionality Reduction

- One reduction technique is dimensionality reduction, which removes attributes from a dataset.

- Removing attributes reduces the dataset's overall size. For instance, suppose you want to explore a person's weight as a function of time using the Weight Log data in Figure 4.25.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/74dc4251-ba85-475c-8ffd-5e1eb3587f17)

  Figure 4.25: Dimensionality reduction example

 # Numerosity Reduction

- Another technique is numerosity reduction, which reduces the overall volume of data.

- Suppose you are working with a decade's worth of Weight Log data from Figure 4.25, and you want to identify the most frequently occurring weight.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/a7246505-41a9-41df-b7e2-74f4ea834a9e)

 Figure 4.26: Numerosity reduction with histograms

 # Aggregation
 
 - Data aggregation is the summarization of raw data for analysis.

#  Transposition

- Transposing data is when you want to turn rows into columns or columns into rows to facilitate analysis.

# Normalization

- In the context of data manipulation, normalizing data differs from our discussion of database normalization in Chapter 3.

# Min-Max Normalization

- If you're curious about how the min-max normalization, consider its mathematical definition:

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/73d84520-1a89-4bc2-936f-dad74a74e41a)

# Parsing/String Manipulation

- Raw data can contain columns with composite or distributed structural issues.

- A composite issue is when a raw data source has multiple, distinct values combined within a single character column.

# Managing Data Quality

# Circumstances to Check for Quality

- There are numerous circumstances where it is appropriate to implement data quality control checks.

- Every stop along the data life-cycle journey can impact data quality.

- Errors during data acquisition, transformation, manipulation, and visualization all contribute to degrading data quality.

  You should recognize the types of quality issues that can occur and have an overarching strategy to ensure the quality of your data.

 - Data Acquisition

- Data Transformation and Conversion

- Data Manipulation

- Final Product Preparation

# Automated Validation

- Many data sources feed analytics environments. While some of these data sources are other computer systems, others depend directly on people.

- Whenever people interact with systems, it's possible to introduce data-related errors.

# Data Quality Dimensions

- It is essential to consider multiple attributes of data when considering its quality. 

- Six dimensions to take into account when assessing data quality are accuracy, completeness, consistency, timeliness, uniqueness, and validity.

- Data Accuracy

- Data Completeness

- Data Consistency

- Data Timeliness

- Data Uniqueness

- Data Validity

  Data Quality Rules and Metrics

- With an understanding of data quality dimensions, you need to consider how to measure each of them in your quest to improve overall quality.

- Let's consider data conformity, which encompasses elements of accuracy, consistency, uniqueness, and validity.

-  When consolidating data from multiple source systems into an analytics environment, one factor you want to assess is the conformity or nonconformity of data.

# Methods to Validate Quality

 - Numerous methods are available for validating data quality.
 
 - These methods range from whether or not your data passes reasonable expectations to statistical methods that look for irregular patterns within your data.
 
 - A sound approach to ensuring and improving data quality is by combining these methods appropriately.

# Reasonable Expectations

- One approach is to determine whether or not the data in your analytics environment meets your reasonable expectations.

# Data Profiling

- Another approach to improving quality is to profile your data.

- Data profiling uses statistical measures to check for data discrepancies, including values that are missing, that occur either infrequently or too frequently, or that should be eliminated.

# Data Audits

- Another method to keep in mind is auditing your data. 

- Data audits look at your data and help you understand whether or not you have the data you need to operate your business.

# Sampling

- Another method for validating data quality is by examining a sample of your data.

- Sampling is a statistical technique in which you use a subset of your data to inform conclusions about your overall data.

# Cross-Validation

- Analysts frequently use existing data to generate predictive models using a variety of statistical methods. 

- Cross-validation is a statistical technique that evaluates how well predictive models perform.

Describe the unique challenge of missing data values. 

- A missing value is the absence of a value. Regardless of the programming language you use to manipulate data, you need additional checks to account for the lack of a value.

Describe why it is crucial to account for data outliers. 

- An outlier is an observation whose value differs significantly from other observations of the same type.

  Describe the difference between data merging and data blending. 

 - Both data merging and data blending combine data sources. 
 
 - However, data merging combines sources programmatically, typically through an ETL operation.

 Differentiate between dimensionality and numerosity reduction. 

 - Dimensionality reduction is a technique for removing attributes that are not relevant for the analysis at hand.

 Describe how you can enforce data validity. 

- Data validity is the data quality dimension that identifies whether a given value falls within an expected range.

# Data Analysis and Statistics

In this chapter, we will learn how to:

- Understand the concepts of statistics.

- Learning about the appropriate descriptive statistical methods.

- Understand the inferential statistical methods.

- Summarize types of analysis and key analysis techniques.

# Fundamentals of Statistics

- One key concept is the definition of a population.

- A population represents all the data subjects you want to analyze.

- Collecting a sample is a cost-effective and time-effective alternative to gathering census data.

- A sample is a subset of the population.

- You analyze samples in terms of statistics.

- A statistic is a numeric representation of a property of a sample.

 Common Symbols in Statistics

- Statistics is all about exploring numbers and performing calculations. 

![Screenshot (7)](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/c832a002-f435-41da-a14d-d5e64f3b5527)

Table 5.1: Common symbols in statistics

- Descriptive statistics focus on describing the visible characteristics of a dataset, using summary statistics, graphs, and tables.

- Inferential statistics, on the other hand, use samples to draw inferences about larger populations.

# Sample

- Size of the population used for study.

# Sample Size

- Total amount of things in a sample.

# Quantitative Data

- Data that is measured in numbers. It deals with numbers that make sense to perform artithmetic calculations with HEIGHT,WEIGHT,MIDTERM SCORE variables.

Categorical Data 

- Refers to the values that place things in different groups or categories.

# Descriptive Statistics

- Descriptive statistics is a branch of statistics that summarizes and describes data.

- As you explore a new dataset for the first time, you want to develop an initial understanding of the size and shape of the data.

- You use descriptive statistics as measures to help you understand the characteristics of your dataset.

# Measures of Frequency

- Measures of frequency help you understand how often something happens.

- When encountering a dataset for the first time, you want to determine how much data you are working with to help guide your analysis.

# Count

- The most straightforward way to understand how much data you're working with is to count the number of observations.

# Percentage

- The percentage is a frequency measure that identifies the proportion of a given value for a variable with respect to the total number of rows in the dataset.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/21711c54-6ad7-417d-b22d-e67f329b95d1)

  Table 5.3: Sample data

  Frequency

- Frequency describes how often a specific value for a variable occurs in a dataset.

- You typically explore frequency when conducting univariate analysis.

# Measures of Central Tendency

- To help establish an overall perspective on a given dataset, an analyst explores various measures of central tendency.

# Mean

- The mean, or average, is a measurement of central tendency that computes the arithmetic average for a given set of numeric values.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/cc25881e-e17c-4bfd-ae16-f6291d2dd571)

 Formula


Median

- Another measurement of central tendency is the median, which identifies the midpoint value for all observations of a variable.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/e0c7dd2b-c962-4138-9b24-4e2e89bc0219)

Odd Numbers Formula

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/755bf1e5-8d26-4390-a00a-fc7b5674a7b2)

Even Numbers

# Mode

- The mode is a variable's most frequently occurring observation.

- Depending on your data, you may not have a mode.

# Measures of Dispersion

 - In addition to central tendency, it is crucial to understand the spread of your data. 
 
 - You use measures of dispersion to create context around data spread.
 
 - Let's explore five common measures of dispersion.

# Range

- The range of a variable is the difference between its maximum and minimum values.

# Distribution

- In statistics, a probability distribution, or distribution, is a function that illustrates probable values for a variable, and the frequency with which they occur.

# Normal Distribution

- The normal distribution is symmetrically dispersed around its mean, which gives it a distinctive bell-like shape.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/1bd7b87f-8bb0-4bb0-ab20-f5155fbb023d)

Figure 5.7: Normal distribution

Skewed Distribution

- A skewed distribution has an asymmetrical shape, with a single peak and a long tail on one side.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/7dc99b9f-80d0-4d50-8bcc-a9d3cec6b6c8)

 Figure 5.8: Right skewed distribution

# Bimodal Distribution

 A bimodal distribution has two distinct modes, whereas a multimodal distribution has multiple distinct modes. 

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/387940b0-61b3-4118-8aad-31ce195732bf)

 Figure 5.10: Bimodal distribution
 
 
# Variance

 Variance is a measure of dispersion that takes the values for each observation in a dataset and calculates how far away they are from the mean value. 

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/c28e0df2-7588-4af4-8e74-8a39af6bb799)

 Variance Formula

 # Standard Deviation

- Standard deviation is a statistic that measures dispersion in terms of how far values of a variable are from its mean.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3b83953b-93b2-45f5-b96e-ca52a763254e)


 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/a0537cd0-1b38-4947-9359-d0762591a3b2)


 Each Sample is Unique

- Keep in mind that each sample from a population is unique.

- Suppose you take two different samples from a population.

# Special Normal Distributions

- The Central Limit Theorem and empirical rule combine to make the normal distribution the most important distribution in statistics.

 Standard Normal Distribution

- The standard normal distribution, or Z-distribution, is a special normal distribution with a mean of 0 and a standard deviation of 1.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/c4251697-3fe9-432f-a576-089888277982)

 Calculating Standardized Scores

 Student's T-Distribution

- The Student's t-distribution, commonly known as the t-distribution, is similar to the standard normal distribution in that it has a mean of 0 with a bell-like shape.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/77f5cf81-8d73-4ae1-a229-1812f8195055)

 Figure 5.12: Standard normal distribution and t-distribution

 # Measures of Position

 - Understanding a specific value for a variable relative to the other values for that variable gives you an indication of the organization of your data.

# Inferential Statistics

- Inferential statistics is a branch of statistics that uses sample data to draw conclusions about the overall population.

# Confidence Intervals

- Each time you take a sample from a population, the statistics you generate are unique to the sample.

- In order to make inferences about the population as a whole, you need a way to come up with a range of scores that you can use to describe the population as a whole.

# Confidence Interval Considerations

- While it is possible to develop a confidence interval for a skewed distribution, our conversation will focus on a normal distribution.

- Presume the sample mean, population standard deviation, and sample size are known.

 ![Screenshot (8)](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/1c033936-fdf7-4075-a99c-217034e8e243)

 Substituting Sample Standard Deviation For Population Standard Deviation

- When calculating confidence intervals, you need to have the standard deviation of the entire population.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/e80314db-f8a1-409a-b22e-38e4a3a90246)

 # Hypothesis Testing

- Data analysts frequently need to build convincing arguments using data.

- One of the approaches to proving or disproving ideas is hypothesis testing.

- A hypothesis test consists of two statements, only one of which can be true.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/4c60ea3f-b9f2-40b1-aa8c-ca2027cbaf92)

 Figure 5.17: Visualizing alpha

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3cfd72d2-256a-41e6-89c2-47eb92514628)
 
 Figure 5.18: One-tailed test

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/e46a359c-d3b9-4312-bbce-112f1c14bf5b)
 
 Figure 5.19: Two-tailed test

 # Simple Linear Regression

- Simple linear regression is an analysis technique that explores the relationship between an independent variable and a dependent variable.

- You can use linear regression to identify whether the independent variable is a good predictor of the dependent variable. 

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/0b77bd07-b8e7-4676-ae10-641a7231b0bf)

  Figure 5.23: Simple linear regression of age and BMI

  From Simple to Multiple Linear Regression

- Note that you explore the relationship between two variables using simple linear regression.

- Multiple linear regression builds on that concept by examining the effect of numerous independent variables on a dependent variable.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/de459c8a-bb15-4e76-b9ce-f9a931a1bb3a)

   Figure 5.26: Highly correlated data

# Analysis Techniques

- Data analysts have an abundance of statistical tools available to explore data in the pursuit of insights.

  Determine Type of Analysis

 - When embarking on a new analytics challenge, you need to understand the business objectives and desired outcomes. 
 
 - This understanding informs the type of analysis you will conduct.

 # Types of Analysis

 - With a clear scope and access to the data you need, you can get on with your analytical work. 
 
 - One of the types of analysis you may be asked to perform is trend analysis. Trend analysis seeks to identify patterns by comparing data over time.

 - In addition to trend analysis, you may also conduct performance analysis.
 
 - Performance analysis examines defined goals and measures performance against them and can inform the development of future projections.

 -  Link analysis is a technique that uses network theory to explore the relationship between data points.

 # Exploratory Data Analysis

 - At the onset of your analysis, you will encounter many datasets for the first time.
 
 - When first exploring a dataset, it's a good idea to perform an exploratory data analysis.
 
 - An exploratory data analysis (EDA) uses descriptive statistics to summarize the main characteristics of a dataset, identify outliers, and give you context for further analysis. 

 - EDA, they typically encompass the following steps:

# Check Data Structure:  
 
 - Ensure that data is in the correct format for analysis.
 
 - Most analysis tools expect data to be in a tabular format, so you need to confirm that your data has defined rows and columns.

# Check Data Representation: 
 
 - Become familiar with the data. 
 
 - In this step, you validate data types and ensure that variables contain the data you expect.

# Check if Data Is Missing:  
 
 - Check to see if any data is missing from the dataset and determine what to do next.
 
 - While checking for null values, calculate the proportion of each variable that is missing.
 
 - If you discover that most of the data you need is missing.

 # Identify Outliers: 

- Recall from Chapter 4 that an outlier is an observation of a variable that deviates significantly from other observations of that variable. 

- As shown in this chapter, outliers can dramatically impact some descriptive statistics, like the mean.

# Summarize Statistics:  

- Calculate summary statistics for each variable. For numeric variables, examples of summary statistics include mean, median, and variance.

 Check Assumptions:  
 
 - Depending on the statistical method you are using, you need to understand the shape of the data.

Differentiate between descriptive and inferential statistics. 

# Descriptive statistics 

- Helps you understand past events by summarizing data and include measures of frequency and measures of dispersion.

- Inferential statistics use the powerful concept of concluding an overall population using a sample from that population.

# Calculate measures of central tendency. 

- Given a dataset, you should feel comfortable calculating the mean, median, and mode.

- Recall that the mean is the mathematical average. The median is the value that separates the lower and higher portions of an ordered set of numbers.

- The mode is the value that occurs most frequently.

- While mean and median are applicable for numeric data, evaluating the mode is particularly useful when describing categorical data.

# Explain how to interpret a p-value when hypothesis testing. 

- Recall that p-values denote the probability that a test statistic is as extreme as the actual result, presuming the null hypothesis is true.

- The lower the p-value, the more evidence there is to reject the null hypothesis.

# Explain the difference between a Type I and Type II error. 

- When hypothesis testing, a Type I error is a false positive, while a Type II error is a false negative.

- Suppose you have a null hypothesis stating that a new vaccine is ineffective and an alternative hypothesis stating that the vaccine has its intended impact.

- Concluding that the vaccine is effective when it isn't is a Type I error. A Type II error is a false conclusion that the vaccine does not work when it does have the intended effect.

# Describe the purpose of exploratory data analysis (EDA). 

- One of the first things you should perform with any new dataset is EDA, a structured approach using descriptive statistics to summarize the characteristics of a dataset. 

- Identify any outliers, and help you develop your plan for further analysis.

Chapter 6

In this chapter, we will learn about:

- The most common tools used for Data Analytics

- The preferred programming languages for Data Analytics 

- The techniques of how analysts and developers interact with databases

# Spreadsheets

- The spreadsheet is the most widely used tool in the world of analytics.

- It is hard to imagine anyone who does not use spreadsheets as part of their work because they provide an intuitive way to organize our data into rows and columns.

- Spreadsheets are productivity software packages that allow users to create documents that organize any type of data into rows and columns. 

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/1b5d289f-d46a-4f13-8f36-f69a13e0edb6)

 Figure 1: Table of data in Microsoft Excel

# Programming Languages

- Programming languages allow skilled software developers to write their own instructions to the computer.

- Allowing them to directly specify the actions that should take place during the analytics process.

# R

- The R programming language is extremely popular among data analysts because it is focused on creating analytics applications.

- R gained rapid traction as a popular language for several reasons.

- First, it is available to everyone as a free, open-source language developed by a community of committed developers. 

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/f18353f6-8dbd-4fdb-b11a-b097998387d5)

 Figure 3: Data analysis using R and RStudio

# Python

- Python is arguably the most popular programming language in use today.

- Python is about the same age as R, but the major difference between Python and R is that Python is a general-purpose programming language.

- Python also has specialized libraries that focus on the needs of analysts and data scientists.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/0efab725-9429-4d4c-b51d-7329fb48869d)

  Figure 4: Data analysis using Python and pandas

 # Structured Query Language (SQL)

- The Structured Query Language (SQL) is the language of databases.

- Any time a developer, administrator, or end user interacts with a database, that interaction happens through the use of a SQL command.

- SQL is divided into two major sublanguages:

The Data Definition Language (DDL) 

- It is used mainly by developers and administrators.

- It's used to define the structure of the database itself.

- It doesn't work with the data inside a database, but it sets the ground rules for the database to function.

 Data Manipulation Language (DML) 
 
- Is a subset of SQL commands that are used to work with the data inside of a database.

- They do not change the database structure, but they add, remove, and change the data inside a database.

  There are three DDL commands that you should know:

- The CREATE command is used to create a new table within your database or a new database on your server.

- The ALTER command is used to change the structure of a table that you've already created.

- The DROP command deletes an entire table or database from your server.

  There are also four DML commands that you should know:

- The SELECT command is used to retrieve information from a database.

- The INSERT command is used to add new records to a database table.

- The UPDATE command is used to modify rows in the database.

- The DELETE command is used to delete rows from a database table.

# Statistics Packages

-  Statistics packages are a great example of this.

-  These software packages go beyond the simple statistical analyses that are possible in spreadsheets.

-  Provides access to advanced statistical environments that are accessible through a graphical user interface and/or a built-in scripting language.

# IBM SPSS

- One of the most popular pieces of statistical software is IBM's SPSS package.

- SPSS is one of the oldest statistical software packages, first released in 1968.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/85176a70-5984-400e-8b0e-a2e2a178652e)

 Figure 7: Analysing data in SAS

 Stata

 - Stata is yet another statistical analysis package that dates back to the 1980s and continues to be updated today. 
 
 - It offers essentially the same features as SPSS and SAS and provides users with both a graphical interface and a command-line interface depending on their personal preference.

   ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/740fbe20-1acb-4d30-a33a-06c0e4f8f343)

 Figure 8: Building a simple model in Stata

# Minitab

- The final statistical software package covered on the Data+ exam is Minitab.

- Once again, Minitab shares most of the same features as SPSS, SAS, and Stata but fits into the same category as Stata.

# Machine Learning

- Moving on from statistics-focused tools, the industry also makes use of a set of graphical tools designed to help analysts build machine learning models without requiring them to actually write the code to do so.
 
# IBM SPSS Modeler

- IBM's SPSS Modeler is one popular tool for building graphical machine learning models.

# RapidMiner

- RapidMiner is another graphical machine learning tool that works in a manner similar to IBM SPSS Modeler.

# Analytics Suites

Analytics tools that fit into two basic models: 

- Programming languages that allow skilled developers to complete whatever analytic task face them and specialized tools, such as spreadsheets, statistics packages, and

- Machine learning tools that focus on one particular component of the analytics process.

# IBM Cognos

- IBM Cognos is an example of one of these integrated analytics suites.

- Cognos Connection is a web-based portal that offers access to other elements of the Cognos suite.

- Query Studio provides access to data querying and basic reporting tools.

- Report Studio offers advanced report design tools for complex reporting needs.

- Analysis Studio enables advanced modeling and analytics on large datasets.

# Microsoft Power BI

- Power BI is Microsoft's analytics suite built on the company's popular SQL Server database platform.

The major components of Power BI include the following:

- Power BI Desktop is a Windows application for data analysts, allowing them to interact with data and publish reports for others.

- The Power BI service is Microsoft's software-as-a-service (SaaS) offering that hosts Power BI capabilities in the cloud for customers to access.

# MicroStrategy

- MicroStrategy is an analytics suite that is less well-known than similar tools from IBM and Microsoft, but it does have a well-established user base.

# Domo

- Domo is a software-as-a-service (SaaS) analytics platform that allows businesses to ingest their data and apply a variety of analytic and modeling capabilities.

# Datorama

- Salesforce Datorama is an analytics tool that focuses on a specific component of an organization's business: sales and marketing.

# AWS QuickSight

- AWS QuickSight is a dashboarding tool available as part of the Amazon Web Services cloud offering.

# Tableau

- Tableau is arguably the most popular data visualization tool available in the market today.

# Qlik

Qlik is another popular SaaS analytics platform, offering access to cloud-based analytics capabilities. 

- The major products offered by Qlik include the following:

- QlikView is the company's original analytics platform that focuses on providing rapid insights.

- Qlik Sense is a more advanced platform providing more sophisticated analytics capabilities (at a higher cost, of course!).

# BusinessObjects

- BusinessObjects is an enterprise reporting tool from SAP that is designed to provide a comprehensive reporting and analytics environment for organizations.

Chapter Review – Exam Preparation

# Describe the role of the spreadsheet in the modern organization. 

- Spreadsheets are productivity software packages that allow users to create documents that organize any type of data into rows and columns.

# Understand how analytics teams use programming languages. 

- Data professionals with coding skills often turn to programming languages to create their software analysis tools.

# Know how analysts and developers interact with databases. 

- Relational databases are the primary data stores used in the modern organization.

- Analysts and developers may interact directly with databases using the Structured Query Language (SQL).

- SQL has two subcomponents. The Data Definition Language (DDL) defines the structure of the database and contains commands to create, alter, and destroy databases and tables.

- The Data Manipulation Language (DML) interacts with the data stored in a database and contains commands to add, retrieve, modify, and delete data.

# Describe the role of statistical analysis software. 

- Statistical analysis software provides access to advanced statistical environments that are accessible through a graphical user interface and/or a built-in scripting language.

# Describe the role of machine learning software. 

- Machine learning packages offer a set of graphical tools designed to help analysts build machine learning models without requiring them to actually write the code to do so.

- These machine-learning tools aim to make machine-learning techniques more accessible.

# Describe the role of data analytics suites. 

- Data analytics suites provide powerful capabilities that cross all phases of an analytics process.

- These tools allow analysts to ingest and clean data, perform exploratory statistical analysis, visualize their data, produce models, make predictions.

# Chapter 7

# Understanding the Business Requirements

- Reports and dashboards both summarize data for end users, but they distribute those summaries in different ways. 

- A report is a static electronic or physical document that reflects information at a given point in time.

- On the other hand, a dashboard is an interactive visualization that encourages people to explore data dynamically.

- Both reports and dashboards are ideal tools for visualizing data content.

- With a pull approach, you publish a report to a known location, like a web page, and let people know the frequency and timing of when the report updates.

- With a push approach, the report is automatically sent to the appropriate people as it becomes available.

-  With the blended approach, you inform people that the report is available while maintaining central control of the report itself.

# Understanding Report Design Elements

- Control: has to do with how you focus the attention of your audience. When someone encounters a dashboard for the first time, one of your goals is to deliver the pertinent information quickly.

- Correctness: makes sure that your information is accurate and that there are no spelling mistakes. Pay close attention to correctness when using corporate names and logos.

- Clarity: refers to selecting the right visualization tool for communicating your message, making sure the visualization is easy to interpret and visually crisp, and using fonts and sizes that are easy to read.

- Consistency: refers to using the same design and documentation elements throughout your report or dashboard to give your visualization a cohesive and complete feel.

- Concentration: refers to using visuals to focus your audience's attention on the most relevant information without overwhelming them with details.

# Report cover page

- When developing a printed report, keep in mind that the first thing people see is the cover page.

- Since the cover page is the first thing a person sees, it is vital that it sets expectations about the observations and insights the reader will find within.

# Executive Summary

- Following a report's cover page is an executive summary.

- The executive summary provides an overview of the report's contents.

# Design Elements

- When developing a report or dashboard, you need to incorporate design elements into your thinking.

- Color schemes, page layout, font size and style, charts, and corporate standards are among the many design elements you should consider.

# Color Schemes

- A color scheme is a limited selection of colors you use when creating a report or a dashboard.

- The first decision to make is whether you need to use a monochromatic color palette or have the flexibility to use more than one color.

# Layouts

- The layout of a report or dashboard determines the arrangement of its component parts. 

- It is crucial to consider approachability when thinking about the design.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/a9d51ac3-cb2b-461a-bd3b-ecbe141b6cdf)

  Figure 7: Nonparallel construction

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/897b206f-dd85-49ef-bb8f-07602268d07d)

 Figure 8: Parallel construction

# Fonts

- When choosing a font style, pick one that is easy for people to read by avoiding ornate fonts.

- After excluding ornate options, you need to decide between a serif or sans serif font style.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/4448af7d-110a-4609-a6c1-55828ac3ffc3)

   Figure 9: Serif and sans serif fonts

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/24e441fd-40f3-4bac-ab80-363ef8e7e081)

   Figure 10: Sample layout elements and font sizes

# Graphics

- Using graphics to present summary information is a practical choice, whether creating a report or developing a dashboard.

- As the saying goes, a picture is worth a thousand words, and visually conveying information with charts helps focus your audience's attention.

# Corporate Reporting Standards

- When developing any type of visualization, be mindful of any existing corporate reporting standards.

- For instance, your organization may have a style guide for reporting.

# Documentation Elements

- People must trust the information in your visualizations.

- To help establish trust, you can incorporate documentation elements, including version numbers, reference data sources, and reference dates.

# Version Number

- A version number is a numeric value that refers to a specific version of a report.

- Version numbers help you keep track of changes to content and layout over time.

# Reference Data Sources

- Reference dates help people understand what to expect in terms of data recency.

- For example, if a report has a daily refresh cycle, the report run date helps people realize when the last data refresh date was.

# Appendix

- When developing a report, use an appendix to include supporting details that are inappropriate to include in the main body.

# Understanding Dashboard Development Methods

- Recall that dashboards are dynamic tools that help people explore data to inform their decision-making.

# Data Source Considerations

- With clarity on what your dashboard needs to contain, you can proceed with identifying data sources.

- Static data: is data that refreshes at some regular interval.

- A typical design pattern is for operational databases to update a data warehouse every night.

- Continuous data: also known as live data, typically comes directly from an operational database that people use to perform their daily duties.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/5e2b2089-d652-434c-90ca-90bb08c368ef)

 Figure 16: Data sourcing flowchart

# Data Type Considerations

- One thing that differentiates dashboards and reports is the fact that dashboards use software as the delivery mechanism.

# Development Process

- After you identify the data sources that will power your dashboard, you must turn your attention to developing the dashboard itself.

- Use wireframes and mock-ups to help build and refine the dashboard's design

- A mock-up extends a wireframe by providing details about the visual elements of the dashboard, including fonts, colors, logos, graphics and styles.

# Delivery Considerations

- Delivery considerations are a crucial part of the development process. 

# Operational Considerations

- Once you have final approval, you proceed with developing the dashboard.

- Similar to the design stage, make sure you include frequent opportunities to gather feedback.

- Once dashboard development is complete, test it thoroughly to verify its functionality.

- Accounting for how you will refresh data is one of the many things to consider.

# Exploring Visualization Types

# Charts

- Charts are one of the foundational methods for visualizing both qualitative and quantitative data.

- There are many chart types, including line, pie, bar, stacked, scatter, and bubble charts.

# Line Chart

- A line chart shows the relationship of two variables along an x- and a y-axis.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/bd33288e-b23f-4b98-a696-5484a2266156)

  Figure 18: Line Chart

# Pie Chart

- A pie chart gets its name from its circular shape where the circle represents 100 percent, and each slice of the pie is a proportion of the whole.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/d03fa9ae-97ae-4c34-9de6-adb9a6d194f5)

 Figure 19: Pie chart

# Bar Chart

- Similar to a pie chart, a bar chart presents categorical data.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/a6c85031-596d-4f76-b298-a09a8c72af94)

  Figure 21: Bar chart by count

 # Stacked chart 
  
- Stacked bar chart, starts with a bar chart and extends it by incorporating proportional segments on each bar for categorical data.

# Scatter Chart

- A scatter chart, or scatter plot, uses a dot for each observation in a data set to show the relationship between two numeric variables.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/7cc0ee5d-b6cb-44ff-ba2e-3bcafb803a95)

 Figure 22: Stacked Bar Chart

 A bubble chart is a scatterplot where the size of each dot is dependent on a third numeric variable. 

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3f4557c1-2e73-40de-9921-4a8511e0aeaf)
 
  Figure 23: Average Duration Scatterplot

 # Histogram

- A histogram is a chart that shows a frequency distribution for numeric data.

- When performing an exploratory data analysis, create histograms for numeric data.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/7d12d63a-cfc4-4eae-84cf-8ae696bd97fb)

  Figure 24: Budget and Box Office Gross Scatterplot

 # Maps

- People frequently use maps to convey the location of a country, town, or individual address.

- Geographic maps are excellent for location-related data.

- A heat map is a visualization that uses color and location to illustrate significance.

- A tree map uses rectangles whose area depicts a proportional representation of hierarchical data.

# Waterfall

- A waterfall chart displays the cumulative effect of numeric values over time.

- Waterfall charts facilitate an understanding of how a series of events impact an initial value.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/e3e52d8d-db24-4edf-b3a2-2551455af8a9)

Figure 35: Headcount Waterfall Chart

# Infographic

- An infographic, which gets its name from the words “information” and “graphic,” is a visualization that presents information clearly and concisely.

 Figure 36: Infographic

# Word Cloud

- A word cloud is a visualization that uses shape, font size, and color to signify the relative importance of words.

# Comparing Report  Types

- There are several report types to choose from, depending on the information you want to convey. 

- When embarking on any reporting project, recall that you first have to identify the audience and their needs.

# Static and Dynamic

- It is imperative to identify whether a report needs to be static or dynamic, as that difference impacts where you get your data. 

- Static reports pull data from various data sources to reflect data at a specific point in time.

- Dynamic reports give people real-time access to information. 

- Using your five-year trend report to inform their analysis, a financial analyst in your company may want to execute a trade.

# Ad Hoc

- Ad hoc reports, or one-time reports, use existing data to meet a unique need at a specific point in time.

# Self-Service (On-Demand)

- Self-service reports, or on-demand reports, allow individuals to answer questions that are unique to them at a time of their choosing.

# Recurring Reports

- Recurring reports provide summary information on a regularly scheduled basis.

- Typically, recurring reports get delivered to their audience immediately after creation.

- For example, a company's sales leader will want monthly, quarterly, and annual sales numbers available regularly.

- Operational reports that organizations use to monitor organizational health and performance.

- From a financial compliance reporting standpoint, if you are a public company in the United States, you need to document annual compliance with the Sarbanes–Oxley Act (SOX).

- From a safety compliance reporting standpoint, you need to comply with the Occupational Safety and Health Act (OSHA) in the United States to ensure the safety of your employees.

# Tactical and Research

- It is vital to identify whether the report you create is for tactical or strategic purposes.

- Tactical reports provide information to inform an organization's short-term decisions.

- Tactical information helps organizations accomplish initiatives like constructing a building.

- A research report helps an organization make strategic decisions.

- To achieve strategic objectives, an organization executes multiple tactical initiatives.

 Chapter 7 Review – Exam Preparation

 Describe the most crucial first steps when developing a visualization. 
 
 - The most important first steps when developing a visualization are identifying the audience and understanding their needs.

 Compare reports and dashboards. 
 
 - Reports are static documents that are distributed physically or electronically. As static documents, they reflect data at the time of creation.

 Identify the best type of chart for a given scenario. 
 
 - Given a specific scenario, select the type of chart that is most appropriate.
 
 - Bar and pie charts work well for categorical data, whereas line charts are excellent for illustrating the relationship between two variables.

  Identify the type of report that should be used in a given scenario. 
  
 - Given a specific scenario, identify the type of report that would meet the requirement. 

  Describe considerations for maintaining data security. 
  
 - When working with static reports, having tight control over how reports get distributed is one approach to ensuring the security of the data in the report.
