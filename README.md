# Data-Analytics

# Module 1

# CHAPTER 1

 1. What the role of a Data Analyst is and the scope of data analytics.

 2. The process of data analytics.

 3. The techniques of data analytics.

 4. The role of data governance.

# WHAT IS DATA ANALYTICS

 - The ultimate role of a data analyst is to transform raw data into actionable insights that guide decision-making processes within an organization. 

 - This involves several key responsibilities and skills.

 - A data analyst helps an organization make informed decisions that can improve operational efficiencies, drive business strategies, and create a competitive advantage. 

 - The goal is to contribute to the organization's success by turning data into a valuable asset that informs and drives decision-making.

1. Data Collection and Preparation:

  - Sourcing data from various channels, including databases, spreadsheets, and external sources,
 
  - Cleaning and organizing the data to ensure it is accurate, consistent, and ready for analysis.

2. Data Analysis:

  - Employing statistical methods, machine learning techniques, or other analytic tools to interpret data,
 
  - Identifying trends, patterns, and correlations that might not be immediately obvious.

3. Data Visualization and Storytelling:

  - Creating visual representations of the data, such as charts, graphs, and dashboards, to make complex information easily understandable,

4. Decision Support:

  - Making recommendations based on data-driven insights to help guide business decisions,
 
  - Providing context around the data, including potential implications and future trends.

5. Collaboration and Communication:

  - Working closely with other departments, such as marketing, finance, and operations, to understand their data needs and provide insights,
 
  - Effectively communicating complex data findings in a clear and concise manner to non-technical stakeholders,

6. Continuous Learning and Adaptation:

  - Keeping up-to-date with the latest industry trends, tools, and technologies in data analysis.
 
  - Adapting to new types of data and analytical methods as the organization's needs evolve.

 Analytics is made possible by modern data, storage, and computing capabilities.

# THE ANALYTICS PROCESS

  Analysts working with data move through a series of different steps as they seek to gain business value from their organization's data. 

 - Data Acquisition

 - Cleaning and Manipulation

 - Analysis

 - Visualization

 - Reporting and Communication

 While we describe the steps of the analytics process as a series of sequential actions.

 It is more accurate to think of them as a set of interrelated actions that may be revisited frequently while working with a dataset.

# Analytics Techniques:

  Analysts use a variety of techniques to draw conclusions from the data at their disposal.

#  Descriptive Analytics

 - Descriptive analytics focuses on exploring and interpreting historical data to provide a comprehensive understanding of past events and trends. 

#  Predictive Analytics

 - Predictive analytics is particularly valuable for businesses when they want to stay ahead of future trends, behaviors, and outcomes.

# Prescriptive Analytics

 - Prescriptive analytics is most valuable for businesses when they are seeking to predict future outcomes and determine the best course of action to achieve specific objectives. 

# DATA GOVERNANCE

 - Data governance programs ensure that the organization has high-quality data and is able to effectively control that data.

# ANALYTICS TOOLS

 - Software helps analysts work through each one of the phases of the analytics process. 

 - These tools automate much of the heavy lifting of data analysis, improving the analyst's ability to acquire, clean, manipulate, visualize, and analyze data. 

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/f09da0c1-e0ba-4ae7-a6e6-d04bd4a18d85)

Data Analytics Excel Spreadsheet



# CHAPTER 2

 1. Understand Domain 1.0: Data Concepts and Environments

 2. Compare and contrast different data types

 3. Compare and contrast common data structures and file formats

# EXPLORING DATA TYPES

 - A data element is an attribute about a person, place, or thing containing data within a range of values.
 
 - Data elements also describe characteristics of activities, including orders, transactions, and events. 




![Screenshot (1)](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/936c362a-4aab-4396-961d-2a7cf267163f)





# TABULAR DATA

 - Tabular data is data organized into a table, made up of columns and rows. 

 - A table represents information about a single topic.

 - Each column represents a uniquely named field within a table, also called a variable, about a single characteristic. 

 - The contents of each column contain values for the data element as defined by the column header.

 - Spreadsheets, including Microsoft Excel, Google Sheets, and Apple Numbers, are practical tools for representing tabular data

 - A relational database management system (RDMS), commonly called a database, extends the tabular model.

# Structured Data Types

 - Structured data is tabular in nature and organized into rows and columns. 

 - Structured data is what typically comes to mind when looking at a spreadsheet. With clearly defined column headings, spreadsheets are easy to work with and understand. 

 - In a spreadsheet, cells are where columns and rows intersect.
 
![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/83f2a93a-34b5-4530-9ea7-f7de6dd03c4b)

# CHARACTER

 - The character data type limits data entry to only valid characters. 

 - Characters can include the alphabet that you might see on your keyboard, as well as numbers. 

 - Depending on your needs, multiple data types are available that can enforce character limits.

# Alphanumeric

 - Is the most widely used data type for storing character-based data. As the name implies, alphanumeric is appropriate when a data element consists of both numbers and letters.

 - The alphanumeric data type is ideal for storing product stock-keeping units (SKUs).

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/0c408cfc-4acd-4bf6-8006-fd81b071bab9)



Table 2.2: Selected character data types and maximum size


# Character Sets

 - When considering alphanumeric and text data types, you need to think about the character set you are using to input and store data when using a database. 

 - Databases use character sets to map, or encode, data and store it digitally.

# Numeric

 - When numbers exclusively make up values for a data attribute, numeric becomes the data type of choice.

# Whole Number

 - The integer, and all its subtypes, are for storing whole numbers. 

# Rational Number

 - In all its variants, the numeric data type is for rational numbers that include a decimal point.

# Date and Time

 - Gathered together under the broad category of date, day of year and time of day are data elements that appear with great frequency.

# Currency

 - Many people use spreadsheets to manage their finances. 

# Strong And Weak Typing

 - Data types define values placed in columns. 

 - Strong typing is when technology rigidly enforces data types. 

 - Databases, discussed in Chapter 3, use strong typing. A database column defined as numeric only accepts numerical values. You will get an error if you attempt to enter characters into a numeric column.

# Unstructured Data Types

 While much of the data we use to record transactions is highly structured, most of the world's data is unstructured. 

 - Unstructured data is any type of data that does not fit neatly into the tabular model. 

 - Examples of unstructured data include digital images, audio recordings, video recordings, and open-ended survey responses. 

 - Binary data types are one of the most common data types for storing unstructured data.

 - Audio data can come from a variety of sources. 

 - Image data can come from a variety of sources.

 - Video data is growing at a similar pace to image data.

 - Large Text is another data type

  # Categories of Data

 - We try to fit data into structured and unstructured categories. 

 - The reality is that the world is not black and white, and not all data fits neatly into structured and unstructured categories. 

 - Semi-structured data represents the space between structured spreadsheets and unstructured videos.

 # Quantitative vs. Qualitative Data

 - Quantitative data consists of numeric values. Data elements whose values come from counting or measuring are quantitative.

 - Qualitative data consists of frequent text values. Data elements whose values describe characteristics, traits, and attitudes are all qualitative.

 # Discrete vs. Continuous Data

 # Discrete Data

 - Numeric data comes in two different forms: discrete and continuous.

 - Discrete data is that it represents measurements that can't be subdivided.

 - You may intuitively think of discrete data as using whole numbers,

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/d2f5c70a-ee86-4e39-8af4-65cadcffe27c)


# Continuous Data

 - Continuous data refers to data that can be measured.
  
 - This data has values that are not fixed and have an infinite number of possible values.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3090ea0c-2670-485e-88d0-064cb0b27503)



# Categorical Data 

 - In addition to quantitative, numeric data, there is categorical data. 

 - Text data with a known, finite number of categories is categorical.

# Dimensional Data

 - Dimensional modeling is an approach to arranging data to facilitate analysis. 

 - Dimensional modeling organizes data into fact tables and dimension tables.

# Common Data Structures

 - In order to facilitate analysis, data needs to be stored in a consistent, organized manner.

 - When considering structured data, several concepts and standards inform how to organize date.

# Structured Data

 - Tabular data is structured data, with values stored in a consistent, defined manner, organized into columns and rows. 

 - Data is consistent when all entries in a column contain the same type of value.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/2db99357-14be-49cf-b037-a924694858b0)

  Figure 2.19: Data Entry Error

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/6f96fe5f-9485-4d98-a4d7-976bbd2af0b4)

 Figure 2.20: Data entry error identified in a summary


# Unstructured Data

 - Unstructured data is qualitative, describing the characteristics of an event or an object. 

 - Images, phrases, audio or video recordings, and descriptive text are all examples of unstructured data.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3e198f8c-15fc-4044-8120-1624aade7bd9)

 Figure 2.22: Unstructured data: log entry

# Semi-Structured Data

 - Semi-structured data is data that has structure and that is not tabular. 

 - Email is a well-known example of semi-structured data.

 - Every email message has structural components, including recipient, sender, subject, date, and time.

# Common File Formats

 - Common file formats facilitate data exchange and tool interoperability. 

 - Several file formats have emerged as standards and are widely adopted. 

 - As a modern data analyst, you will need to recognize all of these formats and be familiar with common use cases for each type.

# Text Files

 - Text files are one of the most commonly used data file formats.

 - As the name implies, they consist of plain text and are limited in scope to alphanumeric data. 

 - When a file is comma-delimited, it is known as a comma-separated values (CSV) file. 

 - Similarly, when a file is tab-delimited, it is called a tab-separated values (TSV) file.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/9d13be63-a54c-4de6-93fc-827ea1e2c368)

 Figure 2.25: Exporting as CSV or TSV

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/12e15ede-d994-4efb-928e-4caf88a1fba3)

 Figure 2.26: Contents of a CSV file

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/bf1af23c-2d7c-4368-a588-15277a5620e5)

 Figure 2.27: Semi-structured CSV

# Fixed-Width Files

 - Before it was common to use delimited files with variable-length columns, flat files were fixed-width. 

 - As illustrated in Figure 2.28. Fixed-width files are more laborious to create since they require a few extra steps.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/ab16021e-fbdc-4a63-9f6c-bef7c4ef07c7)

 Figure 2.28: Fixed-width file

# JavaScript Object Notation

 - JavaScript Object Notation (JSON) is an open standard file format, designed to add structure to a text file without incurring significant overhead. 

 - One of its design principles is that JSON is easily readable by people and easily parsed by modern programming languages.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/0d0808b2-a1db-4985-8a10-50e5917e24af)

 Figure 2.29: Pet data JSON example


# Extensible Markup Language (XML)

 - Extensible Markup Language (XML) is a markup language that facilitates structuring data in a text file. 

 - While conceptually similar to JSON, XML incurs more overhead because it makes extensive use of tags. Tags describe a data element and enclose each value for each data element. 

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/67b83e15-aee6-417f-b808-55e4f9f09663)

 Figure 2.32: Representing a single animal in XML

# HyperText Markup Language (HTML)

 - HyperText Markup Language (HTML) is a markup language for documents designed to be displayed in a web browser. 

 - HTML pages serve as the foundation for how people interact with the World Wide Web. Similar to XML, HTML is a tag-based language.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/90bd006e-7e77-4559-bd3f-9ded416a504a)

 Figure 2.34: HTML table in a browser

# Chapter 2 Summary

 1. Consider the values of what you will store before selecting data types. 

   - Data types are used to store different kinds of values. 

   - When dealing with numeric information, the best option is a numeric data type that can accommodate decimals.


 2. Know that you can format data after storing it.

  - While data types determine how data gets stored, formatting data governs how data will be displayed to a person.


 3. Consider the absolute limits of values that you will use before selecting data types.

   - When selecting data types, consider the range of values that a data element can contain.


4. Explain the differences between structured and unstructured data

  - Individual data elements fall along the structured data continuum. 

  - At one end, there is highly structured, rectangular data.


5. Understand the differences in common file formats

  - Common file formats make it easy for people to read a file's contents and facilitate interoperability between tools.

  - Delimiters separate variable-length fields in a file.

# Module 2

# Chapter 3

# Databases and Data Acquisition

In this chapter, we will learn about:

 - Data Concepts and Environments

 - Identify basic concepts of data schemas and dimensions

 - Understanding the domain of Data Mining

 - Explain data acquisition concepts

 - Explain common techniques for data manipulation and query optimization

# Exploring Databases

# Relational database 

 - Traditional workloads and systems

 - Excels at storing and processing structured data 

 - Consistency rollback

 - Stored procedures

 - Security ,OLAP

 - Ease of backup

 - Real Time data and detailed

 - Relational databases are pieces of software that let you make an operational system out of an ERD

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/1546d49e-3690-420f-9791-15126342e2af)
 
 Figure 3.6: Database schema


# Nonrelational database 

 - Developing application 

 -  Unstructured

 - Key values , Column store , Graph Databases , Doc store Databases

 - Non-relational databases are ideal for handling large volumes of data and are highly efficient.

 - A nonrelational database does not have a predefined structure based on tabular data.

 - Flexibility, Scalability, Cost effectiveness

# Entity Relationship Diagram

- The entity relationship diagram (ERD) is a visual artifact of the data modeling process. 

- It shows the connection between related entities.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3b0178fa-a7a0-4cd3-ac43-bd461962b06e)

  Figure 3.3: ERD line endings

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/3edcc5bc-6e5e-4429-8597-d088486b0c55)
 
  Figure 3.4: Entity Relationship Diagram


 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/e5ba70a2-c48e-4687-a4ba-cc1ef9a4110c)

  Figure 3.11: Foreign key data constraint

# Normalization

- Normalization is a process for structuring a database in a way that minimizes duplication of data. 

- First normal form (1NF) Is when every row in a table is unique and every column contains a unique value. 

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/b2d2f043-685e-44df-8ae3-beb65fc17c1d)

Figure 3.15: Data in first normal form

- Second Form(2NF)In addition to each row being unique, 2NF applies an additional rule stating that all nonprimary key values must depend on the entire primary key.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/a88691cb-5cec-44e2-8731-9cb693793a4b)

 Figure 3.16: Data in second normal form

- Third Form (3NF)Adding a rule stating all columns must depend on only the primary key.

- Evolving Figure 3.16 into 3NF results in Figure 3.17. 

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/bbe31969-c732-4baa-b56c-cadc885e478a)

Figure 3.17: Data in third normal form


# Online Transactional Processing

- OLTP systems handle the transactions we encounter every day. 

![Screenshot (4)](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/e7e65247-2a4a-4b0b-950e-8c2464d3fa1b)

 Figure 3.14: Vet clinic transactional schema

# Online Analytical Processing

- OLAP systems focus on the ability of organizations to analyze data.

- While OLAP and OLTP databases can both use relational database technology, their structures are fundamentally different.

- OLTP databases need to balance transactional read and write performance, resulting in a highly normalized design.

- Typically, OLTP databases are in 3NF.

# Schema Concepts

- The design of a database schema depends on the purpose it serves. 

- Transactional systems require highly normalized databases, whereas a denormalized design is more appropriate for analytical systems.

 A database used for transactions but slower at querying large amounts of data.

 A data mart is a subset of a data warehouse.

 A data lake stores raw data in its native format instead of conforming to a relational database structure. 
  
  -desgined to capture data from structured, unstructured and semi structured.
  
  -Used for large amounts of data

 A data warehouse is used for analytics and reporting.

 -Has no interference and faster processing.
 
 -Designed for OLAP
 
 -Data summarized

 The star schema design to facilitate analytical processing gets its name from what the schema looks like when looking at its entity relationship diagram, as Figure 3.20 illustrates.

 Star schemas are denormalized to improve read performance over large datasets.

![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/a191fb92-af52-43d1-a69e-b159bc7d90e0)

 Figure 3.20: Star schema example


 A snowflake schema query is more complex than the equivalent query in a star schema. 

- Part of the trade-off is that a snowflake schema requires less storage space than a star schema.

- Another design pattern for data warehousing is the snowflake schema. As its name implies, the schema diagram looks like a snowflake.

  ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/6296e988-773e-4608-9ef3-5d9c8a8f50fe)

  Figure 3.22: Snowflake example


 # Dimensionality

 - Dimensionality refers to the number of attributes a table has. 

 - The greater the number of attributes, the higher the dimensionality.

# Handling Dimensionality

- There are multiple ways to design dimensions. Table 3.5 illustrates the start and end date approach. 

- An understanding of this method is required to write a query to retrieve the current price.

  ![Screenshot (5)](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/5687aae9-55c9-4639-8d96-13e3a4338cf4)

   Table 3.5: Product dimension


# Data Acquisition Concepts

- To perform analytics, you need data. 

- Data can come from internal systems you operate, or you can obtain it from third-party sources.

# Integration 

- Data from transactional systems flow into data warehouses and data marts for analysis.

# ETL and ELT

 Extract:  In the first phase, you extract data from the source system and place it in a staging area. 

 The goal of the extract phase is to move data from a relational database into a flat file as quickly as possible.

 Transform:  The second phase transforms the data. The goal is to reformat the data from its transactional structure to the data warehouse's analytical design.

 Load:  The purpose of the load phase is to ensure data gets into the analytical system as quickly as possible.

- Extract, load, and transform (ELT) is a variant of ETL. With ELT, data is extracted from a source database and loaded directly into the data warehouse. 
 
- Once the extract and load phases are complete, the transformation phase gets underway.

# ETL Vendors

 - Whether you choose ETL or ELT for loading your data warehouse, you don't have to write transformations by hand. 
 
 - Many products support both ETL and ELT.

 ![image](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/eef1976c-31a7-4d84-8b40-a531a1eddbb9)

  Figure 3.23: Delta load example

  An initial load occurs the first time data is put into a data warehouse.
 
  Figure 3.23 illustrates a monthly delta-load approach that continues throughout the year.

#  Data collection methods

 # Application Programming Interfaces (API)

- An application programming interface (API) is a structured method for computer systems to exchange information. 
 
- APIs provide a consistent interface to calling applications, regardless of the internal database structure. 

#  Web Services

- Many smartphone applications need a network connection, either cellular or Wi-Fi, to work correctly. 

 # Surverys

 - One way to collect data directly from your customers is by conducting a survey.
 
 - The most simplistic surveys consist of one question and indicate customer satisfaction.

#  Observation

- Observation is the act of collecting primary source data, from either people or machines. 

#  Sampling
 
 - Regardless of the data acquisition approach, you may end up with more data than is practical to manipulate.

  Survey Tools

- Instead of designing a custom application to collect survey data, several survey products let you design complex surveys without worrying about building a database. 

#  Web Scrapping

- Some of the data you want may not be available internally as an API or publicly via a web service.

# Working With Data

# CRUD

- Create new data.

- Read existing data.

- Update existing data.

- Delete existing data.

![Screenshot (6)](https://github.com/ZaahidAbdurahman/Data-Analytics/assets/169241347/524c1543-8154-40cf-8764-8ac89a8f3995)

 Table 3.8: Data manipulation in SQL
 
 Augmenting data from your transactional systems with external data is an excellent way to improve the analytical capabilities of your organization. 

 # Filtering

- Examining a large table in its entirety provides insight into the overall population. 

# Sorting

- When querying a database, you frequently specify the order in which you want your results to return.

# Date Functions

- Date columns also appear in transactional systems. 
 
- Storing date information about an event facilitates analysis across time.

- The most important thing to note is that you have to understand the database platform you are using and how that platform handles dates and times.

# Logical Functions

- Logical functions can make data substitutions when retrieving data. 

- Remember that a SELECT statement only retrieves data. 

- The data in the underlying tables do not change when a SELECT runs.

  
 Boolean Expressions: The expression must return either TRUE or FALSE.

 True: Value return true if return value

 False: Value return false if return value

# Aggregate Functions

- Summarized data helps answer questions that executives have, and aggregate functions are an easy way to summarize data. 

- Aggregate functions summarize a query's data and return a single value.

# System Functions

- Each database platform offers functions that expose data about the database itself. 

- One of the most frequently used system functions returns the current date.

# Query Optimization

- Writing an SQL query is straightforward. Writing a SQL query that efficiently does what you intend can be more difficult. 
 
- There are several factors to consider when creating well-performing SQL.

 Paremetrization

 - Whenever a SQL query executes, the database has to parse the query.
 
 - Parsing translates the human-readable SQL into code the database understands. 

 Indexing

- When retrieving data from a table, the database has to scan each row until it finds the ones that match the filters in the WHERE clause. 

- The process of looking at each row is called a full table scan.

 Execution plan

- An execution plan shows the details of how a database runs a specific query. 

 Data Subsets and temperature tables

- When dealing with large data volumes, you may want to work with a subset of records.

# Chapter 3 Summary 

Describe the characteristics of OLTP and OLAP systems

- The two main categories of relational databases are transactional (OLTP) and analytical (OLAP). 

- Transactional systems use highly normalized schema design, which allows for database reads and writes to perform well. 

- Analytical systems are denormalized and commonly have a star or snowflake schema.

Describe approaches for handling dimensionality 

- It is crucial to keep track of how data changes over time to perform historical analysis. 

- Although an effective date approach is valid, the SQL queries to retrieve a value at a specific point in time are complex.

  Understand integration and how to populate a data warehouse

- The more data an organization has, the more impactful the analysis it can conduct. 

- The extract, transform, and load (ETL) process copies data from transactional to analytical databases.

 Differentiate between data collection methods

- Data can come from a variety of sources.

- An organization may scrape websites or use publicly available databases to augment its data.

  Describe how to manipulate data and optimize queries

 - Analytical databases store massive amounts of data. 
 
 - Manipulating the entire dataset for analysis is frequently infeasible. 
 
 - To efficiently analyze data, understand that SQL has the power to filter, sort, and aggregate  data.
